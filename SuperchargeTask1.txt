We don't know too much about the project, but first of all we need to understand the expected quality level and the consequences of failure. There is a huge difference in testing a calendar and an emergency call app. Let's assume we don't develop anything very "serious" here, so we don't need to be very thorough in our test approach. I'm going to assume also, that this is a fairly small project, with 1 cross functional team working on it.

There are 4 items in this project that we need to test, the backend, the admin site and the 2 apps. I'm going to assume that the same backend will be used for every app and the admin site. Regarding to our focus, let's prioritize these to be able to distribute our testing efforts efficiently. Let's say that the backend is the most important, because everything else depends on it. After that, the mobile apps and finally the admin site, based on the number of users affected by problems (it's a wild guess, the admin site could break everything too, but we need to start somewhere).

For the backend we should use some automation, depending on our maturity we can build a nice and shiny CI/CD system and run unit tests, api tests and static analysis. If we are able, it would be best to measure code coverage and define a required unit test coverage for the whole backend. Of course this could be too much to ask for, but some automation would be really nice to mitigate regression risks. We don't know what is the lifetime of the project, the longer it is, the more cost we can save with the automation strategy. I'm going to assume that the backend will be tested mostly by developers with consultation of the tester(s) on the project. If we don't have the budget for automation, we can test the APIs manually or rely entirely on the end-to-end tests. This would increase our costs on the debug / fix side, but it could be reduced with better logging. For the APIs backward compatilbity can be a huge complication depending on our update strategy.

The testing of the mobile apps could easily be the most complex part of this project and the planning is crucial here, because the variety of parameters could easily lead to unacceptable testing costs. So, we need to define our scope very carefuly. We need to identify device combinations, screen sizes, OS versions, localizations, and possibly many more variables. Generally, we can check the relevant quality characteristics and determine what we really need to test. I would think about installability, performance, security, usability and localization and of course the functionality aspect. We can use automation for mobile testing too, but let's assume we don't need to support many configurations and we can do it manually. Cost could be an important factor too, we have to decide whether we use real devices or emulators, doing field tests, etc. If we need to minimze the cost, I would go with emulators and doing mostly functional/exploratory testing using personas.

For the admin site we can go for manual or automated tests too, angular has some built-in test capabilities we can utilize. But since this was prioritized to low, let's go with only manual testing. Beside the general functionality we cound focus a little bit on security and role management.

Regarding the team, I would go with 1-2 testers, depending on the level of the acceptable quality risks and the skill level of the testers. I assume we are working in some agile way, so the requirements will change during the project. I would not start automation before the requirements are somewhat stable or use automation only on the stable parts.

Milestone-wise, I would think about 2 major releases before UAT with a "full" test. I would schedule them to the end of the second and fourth month. By this we will be able to track our quality during the project and have some stabilization time before the UAT. Between the major releases there should be minor releases at the end of each sprint. In every sprint we need to test at least the developed stories using the acceptance criteria and also some integration / end-to-end tests to ensure that the developed features are working properly with the other parts of the system and there is no regression issue. The found bugs need to be recorded to a issue management system and prioritized into the next sprint accoring their severity. 

About deliverables: at major milestones I would ask for a test report to the management (and customer). For the minor releases it would be enough to have some kind of dashboard about the current finding and progress. Regular deliverables includes bug records, test cases too. The detailedness of the test cases depends on the project, but I would accept logical test cases first and if the project have a long lifetime we can write the details for them. For exploratory testing we can use test charters and checklists.